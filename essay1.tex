\documentclass{article}
\usepackage{geometry}
\usepackage{nopageno}
\geometry{top=1.0in, bottom=1.0in, left=1.0in, right=1.0in}

\newcommand{\ie}{{\em i.e.,}~}
\newcommand{\eg}{{\em e.g.,}~}

\begin{document}
	
\pagestyle{plain}

\textbf{Describe a significant computer science project you have worked on. If you have worked on a major independent research project (such as research for a graduate program), please describe that work here. Give an overview of the problem, explain how you approached key technical challenges, and describe what you gained from the experience. If the project was team-based, be sure to specify your individual role and contributions.}\\

% why genomics
I am affiliated with the AMP Lab at UC Berkeley.
The lab's mission is to use machine learning (\textbf{A}lgorithms), warehouse-scale computing (\textbf{M}achines), and crowdsourcing (\textbf{P}eople) to tackle Big Data problems.
One of my advisors, David Patterson, has argued that it is important to look outside CS for problems to drive innovation in our field.\footnote{http://www.nytimes.com/2011/12/06/science/david-patterson-enlist-computer-scientists-in-cancer-fight.html} % could change this to a reference
Whole-genome analysis, as opposed to targeted analysis\footnote{\eg https://www.23andme.com/}, has emerged as a potentially powerful medical diagnostic tool.
Once available only to the likes of Steve Jobs, whole-genome sequencing has rapidly become more affordable; the wet lab portion of genome analysis will soon fall to \$1000. % citation to steve jobs book/nyt article?
With these developments, the data analysis required to transform the raw output of DNA sequencing machines to a full genome has become the bottleneck\footnote{http://www.nytimes.com/2011/12/01/business/dna-sequencing-caught-in-deluge-of-data.html} and is therefore a compelling target for our lab.

% why alignment
Only 0.1\% of an individual's genome is unique; the rest is common to all humans.
Thus, to process DNA sequencing data, the standard approach relies on a reference genome, which is a 3-billion base (the letters A, T, C, G) string.
The first step in this analysis pipeline is \textit{short read alignment}, where for each read from the sequencer (\ie around 100 bases of DNA), we find the location in the reference genome where it best matches.
For any particular DNA sample, each read will only differ from the reference by a few characters.
My colleagues and I developed the Scalable Nucleotide Alignment Program (SNAP), an algorithm that leverages resource improvements and conserves computation to get better speed and accuracy than current solutions.

% what I did
Our alignment algorithm uses an index of overlapping 20-base substrings of the reference genome.
To align a read, we sample 20-base substrings, \ie seeds, of the read and look them up in the index.
This gives us a list of candidate positions for each read.
The key difficulty in alignment lies in the fact that the genome is not a random string; rather, it is highly redundant.
Thus, reads often align to many locations throughout the genome, and it is expensive to find the best match.
I realized that the algorithm would be much more efficient if we used precomputation to identify the redundant regions of the genome in advance.
Therefore, I designed and implemented a distributed approach for identifying similar substrings throughout the genome.
%If it were simply a matter of exact duplication, we could easily identify it beforehand.
%Then, when we detect that a read matches a duplicate region, we give up early, since an unambiguous alignment is impossible.
%However, the more difficult problem is near duplication; \ie there are many substrings in the genome that are very similar to each other (\eg a few edits apart).
%We still want to identify these in advance; however, since an unambiguous best alignment may exist, we cannot merely quit early.
%Instead, we want to compare against them in aggregate rather than na\"{\i}vely comparing against them all, one at a time.
I use Spark\footnote{http://www.spark-project.org/}, a framework similar to MapReduce, to find pairs of strings whose edit distance is below an empirically-determined threshold.
The strings represent nodes in a graph, and there is an edge between them if their edit distance is sufficiently small.
Then, the connected components in the graph correspond to similar regions.
This is a work-in-progress, and we are excited to determine its impact on the algorithm's overall performance.

% what I got out of it
This project gave me an opportunity to study genomics and exposed me to the potential for personalized medicine.
I was excited to learn that since genomics is increasingly becoming a big data problem, computer scientists are in a good position to contribute.
%I learned about choosing applications to drive research so you don't fall victim to the hammer/nail problem (\ie if you have a hammer, everything looks like a nail).
I have broadened my circle of collaborators to include another student, other professors in the lab, a medical doctor at UCSF, and a researcher at MSR.
This networking not only helps with idea generation but also will help me identify job opportunities.
I was invited to give presentations on this work at UC Santa Cruz, Genentech, and SRI and got feedback from people who are experts in genomics.
We released a technical report on the algorithm a few months ago\footnote{http://arxiv.org/abs/1111.5572}, and we plan to submit a full paper in the next few weeks.
Most of all, it has been exciting to work on a project with the potential to have a big impact.

\end{document}